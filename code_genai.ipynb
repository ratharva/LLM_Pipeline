{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.telemetry import tutorial_running\n",
    "\n",
    "# tutorial_running(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/envs/pytorchEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack import Pipeline\n",
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'content', 'content_type', 'meta', 'id_hash_keys', 'score', 'embedding'],\n",
      "    num_rows: 151\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/envs/pytorchEnv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "doc_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<haystack.components.retrievers.in_memory.embedding_retriever.InMemoryEmbeddingRetriever object at 0x7fc94d4a9ad0>\n",
      "Inputs:\n",
      "  - query_embedding: List[float]\n",
      "  - filters: Optional[Dict[str, Any]]\n",
      "  - top_k: Optional[int]\n",
      "  - scale_score: Optional[bool]\n",
      "  - return_embedding: Optional[bool]\n",
      "Outputs:\n",
      "  - documents: List[Document]\n"
     ]
    }
   ],
   "source": [
    "print((retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generator = HuggingFaceLocalGenerator(model=\"google/flan-t5-large\",\n",
    "                                      task=\"text2text-generation\",\n",
    "                                      generation_kwargs={\n",
    "                                        \"max_new_tokens\": 100,\n",
    "                                        \"temperature\": 0.9,\n",
    "                                        })\n",
    "\n",
    "generator.warm_up()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fc924ae71d0>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceLocalGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "basic_rag_pipeline = Pipeline()\n",
    "# Add components to your pipeline\n",
    "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "basic_rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 144.43it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2949 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/envs/pytorchEnv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a monument\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Taj Mahal?\"\n",
    "\n",
    "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
    "\n",
    "print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.shortcuts import render\n",
    "from django.http import JsonResponse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
